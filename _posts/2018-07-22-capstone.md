
# Genre Explorations and Predictions 
This project started out as an investigation into training a machine learning model to predict genres utilizing data scraped from the spotify api. Further investigation led to a cross examination of how human intuition, particularly my own intuition, compares to machine learning clustering algorithms. 
[Follow work here](https://github.com/ssanowitz24/Capstone---Genre-Exploration-and-Prediction)

The first part of this project is about learning to use spotipy, a python wrapper for the spotify api. Scrapped data was saved as .csv files. Some data munging came next dealing with nulls and duplicated data. 
The second part was the most crucial to this project. The song data from spotify came with multiple genres attached to an artist/song. To make a predictor, each song would need a concise genre to target. This led me to a binning function where each song was chosen carefully into 12 genres. This function added subjectivity to the modeling process as different people will interperet what genres should be binned into. Once all the songs were binned, more munging followed as songs which did not fit into any genre were given NaN values and removed. 

An extensive amount of Exploratory Data Analysis followed. Every feature of each genre was charted in order to see, similarities, differences and to search for outlier values. Seeing each genre's features graphed out would be the first test of the binning function and its effectiveness.

The modeling process than followed. The data was split into a training, validation and testing sets.  Many classification models were employed including, Logistic Regression, Random Forest, Stochastic Gradient Descent, Extreme Gradient Boosting and a Neural Network. These model were fitted on the training set, tunned on validation and ultimatly scored on the testing set. 

Next was exploring how well a machine learning alogrithm, know as k-means cluster, can produce 12 distinct genres. 12 distict clusters were made and treated as genres. Each cluster was given extensive exploratory data analysis, with charts of each cluster's feature. The last part of the project was seeing how the 12 binned genres compared to the 12 clusters.



# Pulling Data
Pulled data from Spotify API. The main input was from the artists.csv, which gave a list of artist id's. Artist id's  as an input to your spotipy search method can pull up many features, like artist_name and track_id. Only one track was pulled per artist. track_id was used to get spotipy audio features, like speechiness and valence. These features were made into a dictionary and appended to a list. There were 35 different pulls and each pull was saved as an .csv file.


```python

#pulled data 35 times, try and except methods were utilized to avoid list indicie errors.
infos_36 = []
for i in artist_list['id'][81320:]:
    sp.max_get_retries = len(artist_list['id'])
    
    
    info = {}
    
    try:
        artist_name = spot.artist(i)['name']
        results_t = spot.search(q= artist_name,limit=1, type='track')
        results_a = spot.search(q=artist_name, limit=1, type='artist' )
        track_id = results_t['tracks']['items'][0]['id']
        track_name = results_t['tracks']['items'][0]['name']
        genres = results_a['artists']['items'][0]['genres']
        aud_feat = spot.audio_features(track_id)  
        danceability = aud_feat[0]['danceability']
        energy = aud_feat[0]['energy']
        loudness = aud_feat[0]['loudness']
        key = aud_feat[0]['key']
        mode = aud_feat[0]['mode']
        speechiness = aud_feat[0]['speechiness']
        acousticness = aud_feat[0]['acousticness']
        instrumentalness = aud_feat[0]['instrumentalness']
        liveness = aud_feat[0]['liveness']
        valence = aud_feat[0]['valence']
        tempo = aud_feat[0]['tempo']
        time_signature = aud_feat[0]['time_signature']
        artist_uri = results_t['tracks']['items'][0]['album']['artists'][0]['uri']
        popularity = results_t['tracks']['items'][0]['popularity']
    except:
        'list index out of range'
    if genres != []:
        print(artist_name)
        info['artist_name'] = artist_name
        info['track_id'] = track_id
        info['track_name'] = track_name
        info['genre'] = genres
        info['danceability'] = danceability
        info['energy'] = energy
        info['loudness'] = loudness
        info['key'] = key
        info['mode'] = mode
        info['speechiness'] = speechiness
        info['acousticness'] = acousticness
        info['instrumentalness'] = instrumentalness
        info['liveness'] = liveness
        info['valence'] = valence
        info['tempo'] = tempo
        info['time_signature'] = time_signature
        info['artist_uri'] = artist_uri
        info['popularity'] = popularity
        
        infos_36.append(info)
    time.sleep(1)
```


# Binning Genres
Using regex and the list of genres provided to me by the spotify api, I was able to bin the many genres below into 12 target genres. While regex was an objective operation, the order of the target genres and which regex words were binned into each genres were not. Some of the genres that were prominent, that I was not knowledgable of, were researched into and binned accordingly. Ultimatly this was a subjective process and people would think different genres should have been binned differently than what is provided below 


```python
#bin all genres into 12 genres using regex, if a genre does not fall into any category
# give it a NaN
def binning(df):
    for i in range(len(df)):
        if df['genre'].str.contains(r'pop|indie|a cappella|doo-wop|boy band|christmas|halloween').loc[i] == True:
            df['target'].loc[i] = 'pop'
        elif df['genre'].str.contains(r'rock|jam band|dreamo|screamo|emo|grunge|pixie|new wave|surf music').loc[i] == True:
            df['target'].loc[i] = 'rock'
        elif df['genre'].str.contains(r'jazz|swing|broadway|blues|r&b|fusion|disney|barbershop|hollywood|show tunes|ragtime|stride|big band| hard bop| post bop|cabaret').loc[i] == True:
            df['target'].loc[i] = 'jazz/blues'
        elif df['genre'].str.contains(r'metal|punk|djent|deathcore|hardcore|thrash').loc[i] == True:
            df['target'].loc[i] = 'hardcore'
        elif df['genre'].str.contains(r'dub|techno|electro|trance|house|step|turntablism|jack swing|edm|room|drum and bass|workout|otacore|glitch|vapor|eurodance|hardstyle|wonky|rave').loc[i] == True:
            df['target'].loc[i] = 'dance'
        elif df['genre'].str.contains(r'country|folk|bluegrass|nursery|americana|western|hawaiian|zydeco|zouk|redneck|children|lift kit').loc[i] == True:
            df['target'].loc[i] = 'country, folk'
        elif df['genre'].str.contains(r'rap|hip hop|lo-fi|chillhop|trip hop|drill|downtempo').loc[i] == True:
            df['target'].loc[i] = 'rap/hip hop'
        elif df['genre'].str.contains(r'classical|romantic|opera|orchestra|movie|baroque|piano|choral|wind ensemble|string quartet| video game|soundtrack|fingerstyle|chiptune|epicore|violin|flamenco|filmi|marching').loc[i] == True:
            df['target'].loc[i] = 'classical'
        elif df['genre'].str.contains(r'funk|soul|disco|reggae|afropop|azontobeats|ska|opm|soca').loc[i] == True:
            df['target'].loc[i] = 'old school dance'
        elif df['genre'].str.contains(r'mexican|tejano|norteno|cumbia|banda|mariachi|reggaeton|urbano|latin christian|bossa|bolero|salsa|cubaton|rumba|boogaloo').loc[i] == True:
            df['target'].loc[i] ='mexicano/latino'
        elif df['genre'].str.contains(r'gospel|worship|praise').loc[i] == True:
            df['target'].loc[i] = 'gospel/worship'
        elif df['genre'].str.contains(r'focus|sleep|relax|mellow|ambient|chill lounge|meditation').loc[i] == True:
            df['target'].loc[i] = 'focus/sleep'
        else:
            df['target'].loc[i] = np.NAN
binning(df)
    


# EDA
For analysis of the data, all the genre's features were graphed to see the differences and similarities between the genres. Looking at the ranges of each genre's feature was to look for big differences between genres.  A histogram of every genre's feature was employed to see the distributions . This is a great way to look for what makes each genre unique and to find outlier values. 




![png](/images/capstone_notebook_1_files/capstone_notebook_1_36_0.png)





![png](/images/capstone_notebook_1_files/capstone_notebook_1_39_0.png)



```python
plt.figure(figsize=(20,9))
plt.bar(df.target, df.acousticness)
plt.tight_layout
plt.show();
```


![png](/images/capstone_notebook_1_files/capstone_notebook_1_40_0.png)





### Acousticness
Measure of how compressed the song is


```python
plt.hist(df[df.target=='pop'].acousticness);
```


![png](/images/capstone_notebook_1_files/capstone_notebook_1_53_0.png)




### Danceability
Measure of a steady beat


```python
plt.hist(df[df.target=='pop'].danceability);
```


![png](/images/capstone_notebook_1_files/capstone_notebook_1_67_0.png)


### Energy
Measure of busyness


```python
plt.hist(df[df.target=='rap/hip hop'].energy);
```


![png](/images/capstone_notebook_1_files/capstone_notebook_1_80_0.png)

otebook_1_files/capstone_notebook_1_90_0.png)


### Instrumentalness
Measure of Instrumentation Vowel Sound Prominence


```python
plt.hist(df[df.target=='old school dance'].instrumentalness);
```


![png](/images/capstone_notebook_1_files/capstone_notebook_1_92_0.png)


### Liveness
Measure of Audience Noise


```python
plt.hist(df[df.target=='old school dance'].liveness);
```


![png](/images/capstone_notebook_1_files/capstone_notebook_1_105_0.png)



### Loudness
Change in sound pressure measured in decibels


```python
plt.hist(df[df.target=='old school dance'].loudness);
```


![png](/images/capstone_notebook_1_files/capstone_notebook_1_118_0.png)



### Speechiness
Measure of Non-Vowel Sounds


```python
plt.hist(df[df.target=='old school dance'].speechiness);
```


![png](/images/capstone_notebook_1_files/capstone_notebook_1_131_0.png)




### Tempo
Measure of Speed of song in beats per minute


```python
plt.hist(df[df.target=='old school dance'].tempo);
```


![png](/images/capstone_notebook_1_files/capstone_notebook_1_144_0.png)




 ### Valence
 Measure of Posivity in a Song


```python
plt.hist(df[df.target=='old school dance'].valence);
```


![png](/images/capstone_notebook_1_files/capstone_notebook_1_159_0.png)




```python
#investigation of correaltion
plt.figure(figsize=(15,10))
plt.scatter(df.tempo, df.energy,c='g', alpha=0.3)
plt.xlabel('Tempo in bpm')
plt.ylabel('Energy')
plt.title('Tempo vs. Energy')
plt.show()
```


![png](/images/capstone_notebook_1_files/capstone_notebook_1_171_0.png)



```python
#investigation of correaltion
plt.figure(figsize=(15,10))
plt.scatter(df.loudness, df.energy,c='g', alpha=0.3)
plt.xlabel('Loudness in db')
plt.ylabel('Energy')
plt.title('Loudness vs. Energy')
plt.show()
```


![png](/images/capstone_notebook_1_files/capstone_notebook_1_172_0.png)



```python
#investigation of correaltion
plt.figure(figsize=(15,10))
plt.scatter(df.tempo, df.danceability,c='g', alpha=0.3)
plt.xlabel('Tempo in bpm')
plt.ylabel('Danceability')
plt.title('Tempo vs. Danceability')
plt.show()
```


![png](/images/capstone_notebook_1_files/capstone_notebook_1_173_0.png)


### Outliers
One of the more curious cases were the data points with a zero for time siganture and/or tempo. These in fact were not songs at all, but more along the lines of sound effects, noises to aid in sleep or ambient soundscape music. These examples were most prevalent in the focus/sleep target genre






# Modeling
A number of different classification models were used to predict 12 target genres. In order to make these targets applicable for modeling some preprocessing was utilized. In this case label encoder was utilized to make every genre from 0-11 in alphabetical order, so classical music was represented with a 0 and rock music was represented with an 11. The data was than split into a training, validation and a testing set to simulate how a model would perform on unseen data. Using various models to fit on the training set, tune on the validation set and finally score on the testing set. The metric utilized for optimization for the most part was accuracy, though a few other metrics were investigated. 


# Random Forest

A tree method seemed intuitive in predicting genres based on impurity splits. The model's hyperparameter were optimized using Randomized Search Cross Validation. Realizing the model's performance improved with more trees, the n_estimators were than set to 5000, this improved the score but slowed down my model considerably. I tried more than 5000 trees and sometimes my model would produce even better scores, but alot of the time my kernel would die, so 5000 was chosen for its consistency. Random Forest has a great attribute in feature importance which show what features were most effective at getting splits with the lowest impurity.


```


![png](/images/capstone_notebook_1_files/capstone_notebook_1_200_0.png)




```python
# Look at more metrics
print(classification_report(y_test, y_hat, target_names=target_names))
```

                      precision    recall  f1-score   support
    
           classical       0.62      0.74      0.67       666
        country/folk       0.59      0.07      0.12       384
               dance       0.58      0.44      0.50       924
         focus/sleep       0.70      0.39      0.50       231
      gospel/worship       0.56      0.17      0.26       136
            hardcore       0.48      0.26      0.34       375
          jazz/blues       0.41      0.23      0.30       648
     mexicano/latino       0.53      0.17      0.26       359
    old school dance       0.59      0.07      0.12       231
                 pop       0.47      0.82      0.59      3050
         rap/hip hop       0.48      0.42      0.45       682
                rock       0.47      0.18      0.26      1077
    
         avg / total       0.51      0.49      0.45      8763
    



# Weighted Random Forest

Noticing my previous random forest was predicting too many of the majority class and penalizing all the minority classes, I decided to alleviate it with a weighted random forest. Out of two different weightings, the class weight norm out perfomed the class weight predicted. The class weights were calculated by using the class distributions. For example, in the actual distribution there are 4.5 pop songs for every classical song, so classical gets a 4.5 times weighting. In the predicted distribution there are 6.7 pop songs for every classical song, so classical gets a 6.7 weighting ditribution. Pop, being the majority class always get a class weighting of 1.  Though it did not have the desired effect learning and experimenting with class weighting was worth it.



# Logistic Regression
Logistic Regression has a great interpretabilty feature in that it calculates betas used in fitting the training data. Though this model did not produce the best score, it produced great graphs of what features are prominent in a genre. A Grid Search Cross Validation was used to optimize the hyperparameters. 




### Graph of Genre Betas


```python
betas.loc['classical'].plot(kind='bar', color='red', title='Classical Betas',grid =True, figsize=(8,6));
```


![png](/images/capstone_notebook_1_files/capstone_notebook_1_236_0.png)



# SGD Classifier
This model was used to try and speed the modeling process up computationally. This model can optimize over many loss functions and take many different penalties. The hyperprameter were tuned with Randomized Search Cross Validation. This model did score the worse out of all my models.

# XGBoost
XGBoost is famous for winning many kaggle competitions. This model is very versatile, for classification we can use a tree method. The hyperparameters were optimized with Randomized Search Cross Validation. The model already has regularization built in to it to account for overffiting. This model was my second best model behind Random Forest. 

# One vs Rest
A model which creates a binary output for each class using an estimator of one's choice. For this model I used Logistic Regression again, but this time it runs 12 Logistic Regression, one for each class(target). While this model did not score well like Logistic Regression, it produced nice visuals of confidense and probabilty densities.

# Neural Net

In an attempt to see if I could really make my model any better with the features currently set up the way they are I used a neural network. The neural network had 3 hidden layers and an output layer with softmax for multiclassification. The key here was utilizing early stopping, which out of 100 set epochs stopped after usually around 10 epochs, meaning my model could not produce a better score on my validation data. This show my model could not be improved much beyong these scores without a different approach or different features.


# Random Forest with NLP  added to models via song titles
Though against the spirit of the project to use pure sound data, I tried some NLP by vectorizing the words in the song titles using CountVectorizer. It gave me a very simialr score to the random forest without NLP. This took a very long time to fit and was not computationally worth the time for a similar score.


# Clustering Audio Features
Since none of my models could produce an accuracy of above 50%, I invedstigated how a cluster method known as k-means would split the data into 12 clusters. The clusters were treated as targets and predicted on using various Random Forest models with different features. Tempo and Loudness proved to be a point of interest as they were the only features not on a 0 to 1 scale. The clustering algorithm would basically split the clusters on different tmepo and loudness ranges as seen below. Each cluster was given extensive exploratory data analysis as to learn what features were featured in a particluar  cluster. Finally the original binning target genres distributions in each cluster are shown as a cross reference of human intuition to the machine model.



### Distribution of target genres in tempo clusters
Of note the only genre that leads in distribution that is not pop is classical


```python
df[df.predicted == 0]['target'].value_counts(normalize=True).plot(kind='bar', title= 'Tempo: 146.496 - 163.168', color ='blue');
```


![png](/images/capstone_notebook_1_files/capstone_notebook_1_346_0.png)


# Remove Tempo and Cluster
With tempo removed the feature importance of loudness went close to 1, so k-mean clustered made  clusters based on loudness ranges.


# No loudness, mode, time_signature
Without tempo and loudness all the features are on a 0 to 1 scale. This led to the most intriguing results. With the leading feature importance of valence at close to 0.25 there was no clear cut way the k-means clustered the data. This led to exploratory data analysis on each cluster. The top four feature importance's distribution are shown for each cluster via histograms of valence, acousticeness, instrumentalness and energy. Finally the cluster are cross referenced with the 12 target genres form the binning function.



### Histograms of Valence, Acousticness, Instrumentalness, Energy


```python
# four histograms of  valence, acousticmess, instrumentalness and energy, cluster 11
df[df['predicted'] == 11][['valence','acousticness','instrumentalness', 'energy']].plot(kind='hist', subplots=True, title='Genre_11',figsize=(10,8));

```


![png](/images/capstone_notebook_1_files/capstone_notebook_1_396_0.png)



### Cross Referenced Value Counts
Of note besides pop the only other genres to lead in a cluster were dance and classical.


```python
df[df['predicted'] == 11]['target'].value_counts(normalize=True)
```




    pop                 0.426804
    rock                0.139105
    rap/hip hop         0.083531
    dance               0.079295
    mexicano/latino     0.074212
    jazz/blues          0.055066
    country, folk       0.047442
    old school dance    0.046764
    hardcore            0.029651
    classical           0.008980
    gospel/worship      0.006100
    focus/sleep         0.003050
    Name: target, dtype: float64




# Conclusion

To make a genre predictor with the current features from the Spotify Api while also binning the data into 12 target genres produced inaccurate models. Many of the genres overlap on many features and made it quite difficult for a machine learning model to decifer between the various genres. The other problem was unbalanced classes, the pop music genre was about 35% of the total music out of 12 genres. The machine learning models favored the majority class, and penalized the minority classes, predicting up to 60% of the validation data was pop music. Looking for guidance in k-means cluster led to investigation of how a machine learning model could make 12 distinct clusters of music.In my last cluster, with only zero to one scaled fesatures, the only genres that were able to lead a cluster in population besides pop music were dance and classical. 

Looking into why dance and classical were favored over every other genre, we notice that clusters where classical music was the most populous we notice high in acousticness and instrumentalness and genrally lower in valence and energy. This is mostly due to large amount of instrumental music found in classical, the lack of a steady beat in classical and the lack of compression in classical music which leads to a high acousticness rating. The two clusters where dance music were most prevalent had high energy and instrumentalness and low in acousticness. They differed on one cluster had high valence and one had low valence. This is due to the amount of instrumental dance music, dance music is recorded quite loudly giving it high energy and due to the high amount of compression in the dance recording in has very low acousticness. To make a good genre predictor using machine learning models we need less genres for the models to predict or better features that are more precisly in tune with the differences in songs to the human ear. In the end music is subjective to the human experience and how we label music has cultural and historical connotation that a machine learning model can not truly grasp.
